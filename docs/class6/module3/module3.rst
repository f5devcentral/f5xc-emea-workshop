##############
F5 AI Red team
##############

Even before protecting a **GenAI** application we need to understand how vulnerable it is.

**F5 AI Red Team** is a structured methodology combining human expertise with automation and AI tools to uncover safety (of users), security (of operators), 
trust (by users and partners), and performance gaps in systems that incorporate GenAI components. 
It involves simulating adversarial behaviors against Generative AI systems—like LLMs—to uncover vulnerabilities related to security, safety, and trust. 
By "thinking like an attacker" flaws can be identified before they cause real-world harm.


**Module 2 - All sections**

.. toctree::
   :maxdepth: 1
   :glob:

   lab*/lab*

